{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JazQb5OJn_gR"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from huggingface_hub import login, upload_file\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "from huggingface_hub import Repository\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import TrainingArguments, Trainer, BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sqyh2jqIgGxp"
   },
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOSO2RphkqV9"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "dataset = pd.read_csv(\"students_final (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDnCuSgV5bmO",
    "outputId": "c7bbb359-1a73-4384-d39c-161fe967bd8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 235\n",
      "Columns: Index(['Age', 'Gender', 'field', 'Role', 'Exam preference',\n",
      "       'Expression of understanding', 'MCQ : Limited scope',\n",
      "       'MCQ : Easy to guess answers', 'MCQ : Misleading options',\n",
      "       'MCQ : Time Constraints', 'Written : Time-consuming ',\n",
      "       'Written : Requires more effort', 'Written : Subjective grading ',\n",
      "       'Written :  Harder to revise', 'More Preparation time',\n",
      "       'MCQ : memorization or understanding', 'MCQ : encouraging guessing ',\n",
      "       'Stress', 'Chance for higher score', 'confidence MCQs vs. written',\n",
      "       'MCQ : Grades', 'Written : Grades', 'Studying strategy',\n",
      "       'MCQ : Suitablity', 'MCQ : Suitablity examples', 'MCQ : frequency',\n",
      "       'Written : frequency', 'Evaluating critical thinking',\n",
      "       'MCQ : Fairer grading', 'MCQ : Why fairer grading',\n",
      "       'More time-efficient', 'Suggestions Y/N', 'Suggestions',\n",
      "       'MCQ_Suitability_standardized', 'Chance for higher score_category',\n",
      "       'MCQ : Why fairer grading_category'],\n",
      "      dtype='object')\n",
      "Sample Data:\n",
      "       Age  Gender                       field      Role  \\\n",
      "0  18 - 24  Female  Computer and Data Science.  Student.   \n",
      "1  18 - 24  Female  Computer and Data Science.  Student.   \n",
      "2  18 - 24  Female  Computer and Data Science.  Student.   \n",
      "3  18 - 24  Female  Computer and Data Science.  Student.   \n",
      "4  18 - 24  Female           Tourism & Hotels   Student.   \n",
      "\n",
      "                   Exam preference  \\\n",
      "0  MCQ (Multiple Short Questions).   \n",
      "1           A combination of both.   \n",
      "2  MCQ (Multiple Short Questions).   \n",
      "3  MCQ (Multiple Short Questions).   \n",
      "4           A combination of both.   \n",
      "\n",
      "                         Expression of understanding MCQ : Limited scope  \\\n",
      "0  MSQ, as it focuses on quick recall and underst...       1 (Very easy)   \n",
      "1          Both together, for a balanced assessment.       1 (Very easy)   \n",
      "2  Written questions, as they allow me to explain...                   3   \n",
      "3  Written questions, as they allow me to explain...                   2   \n",
      "4  Written questions, as they allow me to explain...                   4   \n",
      "\n",
      "  MCQ : Easy to guess answers MCQ : Misleading options MCQ : Time Constraints  \\\n",
      "0                           2                        3                      4   \n",
      "1               1 (Very easy)       5 (Very difficult)                      2   \n",
      "2               1 (Very easy)                        3          1 (Very easy)   \n",
      "3               1 (Very easy)                        3                      4   \n",
      "4                           4                        4                      4   \n",
      "\n",
      "   ... Written : frequency Evaluating critical thinking MCQ : Fairer grading  \\\n",
      "0  ...                   3                         MCQs                    3   \n",
      "1  ...                   2                         MCQs                    5   \n",
      "2  ...                   3                         MCQs                    2   \n",
      "3  ...                   4                         MCQs                    4   \n",
      "4  ...                   2                         MCQs                    3   \n",
      "\n",
      "                            MCQ : Why fairer grading  \\\n",
      "0         MCQs offer clearer path to a good grade.\\n   \n",
      "1  Clear options provide unbiased measure of know...   \n",
      "2                                Because of cheating   \n",
      "3  Because of cheating in McQs are easier than wr...   \n",
      "4            MCQs offer consistent, quick grading.\\n   \n",
      "\n",
      "          More time-efficient Suggestions Y/N  \\\n",
      "0                        MCQs             Yes   \n",
      "1  Both are equally efficient              No   \n",
      "2                        MCQs              No   \n",
      "3                        MCQs              No   \n",
      "4  Both are equally efficient              No   \n",
      "\n",
      "                                         Suggestions  \\\n",
      "0  multiple-choice questions should be clear and ...   \n",
      "1                                      No suggestion   \n",
      "2                                      No suggestion   \n",
      "3                                      No suggestion   \n",
      "4                                      No suggestion   \n",
      "\n",
      "  MCQ_Suitability_standardized Chance for higher score_category  \\\n",
      "0                  Theoretical               Depends on Subject   \n",
      "1                Technology/CS                    Higher Chance   \n",
      "2                         STEM                    Higher Chance   \n",
      "3                Technology/CS               Depends on Subject   \n",
      "4                         STEM                    Higher Chance   \n",
      "\n",
      "   MCQ : Why fairer grading_category  \n",
      "0                 Clear Expectations  \n",
      "1                  Objective Grading  \n",
      "2                 Structured Options  \n",
      "3                 Structured Options  \n",
      "4                 Consistent Scoring  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Rows:\", len(dataset))\n",
    "print(\"Columns:\", dataset.columns)\n",
    "print(\"Sample Data:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbZmbGe054xh"
   },
   "outputs": [],
   "source": [
    "suggestions_data = dataset[['Suggestions']].dropna()\n",
    "suggestions_data = suggestions_data[suggestions_data['Suggestions'] != 'No suggestions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6uIxhjp5-7D"
   },
   "outputs": [],
   "source": [
    "suggestions_data = suggestions_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVF88rYu68Mc",
    "outputId": "d1f14af2-995f-4e72-b120-6a51dc9e95fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows After Cleaning: 46\n",
      "                                          Suggestions\n",
      "0   multiple-choice questions should be clear and ...\n",
      "1                                       No suggestion\n",
      "11  Offer more practice MCQs to boost confidence a...\n",
      "13  Make written questions measure the level of un...\n",
      "19  More time should be allocated for written asse...\n"
     ]
    }
   ],
   "source": [
    "# Check the cleaned dataset\n",
    "print(f\"Total Rows After Cleaning: {len(suggestions_data)}\")\n",
    "print(suggestions_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKHgreya9wkR"
   },
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset format and rename column\n",
    "hf_dataset = Dataset.from_pandas(suggestions_data.rename(columns={\"Suggestions\": \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_wBeopj-Ccf"
   },
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "validation_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thnRBHAep8li"
   },
   "source": [
    "# Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "d77877091a3c40b08ad8c71ec2e15c8c",
      "0e2094dee28d42aabe3abecd37027749",
      "43751cb476f64faea842a794582a36e9",
      "d259941fd3824dbeaf2b6dff721898aa",
      "59473d48623a45b9bb65db662f2ce5a4",
      "6a7495a8a4cb4579b6c6fab64ae7a634",
      "531dc4f6cd2f4d0db9c69fdc975b8a66",
      "1675dcc6a6bd48f0861c0e1bfc28a05a",
      "151250b4da49451ab0a2fbfdd7451104",
      "30d1fbc9276943dc90267f70d2c9f1cd",
      "957ce94b66d640fd844adbb518cdf3e1",
      "aef21b6194ea4a5fb231d6cb5965ff9c",
      "509801c396f2411c807d6f13bec4a13b",
      "a2d26fd6eac1450396c2428dc0e62532",
      "77aff3cd0ba34767b543095eb031bb89",
      "5963aec611b24d73882328b10520a9ca",
      "1341044287d242eab8101074ed4d5dd4",
      "d4e85d8a621c4222ae9e39444a48b1fc",
      "92853caf91c540ac83c1cd3459d2476c",
      "053fb9ba702b4d228e63ee3693663f82",
      "76fdbc03d63a446a93f783aecd4ad58a",
      "5facc6dff433457694efcc832671da49",
      "557f1629b6484c8d906b625fb24c2da6",
      "8ff37dcb46d549f099082b68b996800f",
      "1c0c6af2675141d3a975eea76d5723b7",
      "6be24986d7d14810bd6d9745aa9d0e09",
      "89c3107db21e435ca98210cb8e0b4844",
      "0ab6940e62fd4b9589a0effad8f792ac",
      "e8d7c15c9fb44a51be30bffd750dc938",
      "97ab5dff58c84d4f832153df0d6a8dd2",
      "39814d30e53c48bc9076a048d3875eb6",
      "b09574a660fc40bfb1124f0eb018414d",
      "68d89d6bb1014a77b0cd42cc201b129b",
      "8d64dcfced9d453f8e5d79333c1fd765",
      "d8cd4574ff094429a1b618adc905cdc3",
      "662cd1cd9c89433d92180c82af889e76",
      "bd6bf114a0e24b4ba2234e7287a1886e",
      "048f4113601a4d588dfba082ee263d5b",
      "ad6bd8418dbb472c8ee7637e39093d48",
      "26da53c6b0944a278d7267e235ebd000",
      "8d9022934191425b8fc7b0cc23fed435",
      "6569a0b56cce4ce69360d1d7a1ec2b06",
      "97c69859ff864735a787d245333b9a60",
      "9c412a4d0308432aac25b05b083563bd"
     ]
    },
    "id": "ESL2_QfTkqnf",
    "outputId": "40ea84e3-efac-40c4-b961-895db94822b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77877091a3c40b08ad8c71ec2e15c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef21b6194ea4a5fb231d6cb5965ff9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557f1629b6484c8d906b625fb24c2da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d64dcfced9d453f8e5d79333c1fd765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility with GPT-2\n",
    "# Set the pad_token to the eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=64, num_return_sequences=20) # Maximum sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "782eeb5dc4fc416abf2230502e5c8457",
      "1874e664506240cf8e9b1cee23c93324",
      "7fceab5ed3fe4fbaa88541a28bc2b6df",
      "d6b87821602e463da25b91e8bb19c9eb",
      "fc58840e921d471e8338ca21ba934fbe",
      "07ccc3d0ad2049f2bf27f1f77eb8fa55",
      "7a0a1594664c467e9aed61d5fc6ee5b0",
      "f8359f0118654526a1a5efa5fa15407b",
      "dedd50a83ac6488d98f05963ef89e349",
      "8581b45a97704e5c810e7065d67b9990",
      "f2b9e6c092834871b814616ca7d003b0",
      "905ebc9ce3c44371bfa3e24924f8175a",
      "3072f4f56b0242fe9ee5646db0253d3a",
      "51e37b24f8984c4e85e84dbbfb382028",
      "7cb1375e8b30488d8aa05baeae94e57d",
      "2682599b492c4138a118c61f64e4e7be",
      "b5cc91f35b524f94aa465f91ac2b9df0",
      "2f742ed35c0e4953a937a3280b2a529f",
      "2a22279a907346b7b19dbf55791603cb",
      "9c6425c342654de3a4d31091e52de8f7",
      "3e76645626e04ea19eac63e27bb57cf6",
      "40a4fad81435435fa86cb11f7f48e749"
     ]
    },
    "id": "0-2xma3VKP0Y",
    "outputId": "81106b64-5dae-4f1f-e905-86061986e977"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782eeb5dc4fc416abf2230502e5c8457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905ebc9ce3c44371bfa3e24924f8175a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_valid = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns (keeping tokenized data only)\n",
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "tokenized_valid = tokenized_valid.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "81be654579a3405a8ed6b4ffea55710b",
      "95ff82b87acb4c1bb2fc9f146e4057a9",
      "fbb81c4c1d374cbda18ef51f7a720c22",
      "923bbac8f8d240eab9f57cce26b8bcd6",
      "07b797f0c3104fe0bd29725e99bd0c9d",
      "b9ac28fdf5bd47829d36654c260582b6",
      "fd914af1659d4ed4b5a4686bd325e9db",
      "87460bfc49e84a2288fc6afde1c02e07",
      "e928aea57ceb4548a2bac29aef0b7bff",
      "fa6ec823935747fca439d5507aaa55d6",
      "80d68ac803f64d21b3c0121731009d03",
      "ef6578768c5342969a1acd5441120a5e",
      "f37f101d23d54508a46e4ac510d89cc5",
      "3c33f1dd0f7f4f528187e1eab784fe29",
      "8286309bf52148ce981630653db381a3",
      "59b677ff9ea44ad2a0f5c080d7a34001",
      "671ef667723542a4b828914cdf9999fa",
      "01ab4404203b4066a8cae25f3a7b0f4b",
      "77dcb71ea547469c804db18d129209d9",
      "69f8813f44304a198338e92a89122738",
      "c298bdf9fab94c8fb265c89e167f01c3",
      "64f134ce0e704b78b517ab900f72a4a6"
     ]
    },
    "id": "iWv2ObQEKSV1",
    "outputId": "5dace623-1d0a-4d80-875a-c5bffcf6d892"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81be654579a3405a8ed6b4ffea55710b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6578768c5342969a1acd5441120a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add labels by copying input_ids\n",
    "# The labels are simply the input_ids because the model learns to predict the next token.\n",
    "# Step 6: Add labels to the dataset (important for computing the loss during training)\n",
    "def add_labels_to_input(example):\n",
    "    example['labels'] = example['input_ids']\n",
    "    return example\n",
    "\n",
    "tokenized_train = tokenized_train.map(add_labels_to_input)\n",
    "tokenized_valid = tokenized_valid.map(add_labels_to_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7myUwV1KUu8"
   },
   "outputs": [],
   "source": [
    "# Step 7: Set up the data collator for language modeling (since GPT-2 is a causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # GPT-2 is a causal language model, not a masked language model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpE5GTOLqQoI"
   },
   "source": [
    "# Fine-Tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "b3460227a77f41bb89a9cc2eb9733f91",
      "0f923dc3429a493fbf11d22c28fb7ae0",
      "b0aea92b3678431dbffff9c67c81b279",
      "6c848924bdf1470daece07f0abf05d75",
      "b9343603079d4ac6a34b32b69a9ba0bf",
      "66ecf7384feb4608969e2ebfa874ed0c",
      "22097b263a4f4401b8b9e26774959708",
      "02c2e7fc86a34e3aa391a179723e629e",
      "84844b2b0c6d4ee5aa6a3b48c01f587a",
      "36bc48275a934a5a89338b2bc755462f",
      "41d5cca172614f9c9ef9e4bcfbca8277",
      "98671844d935406cb04c1b197b72cda4",
      "45446db7e9504f0d92448725749a8750",
      "b79d10e59c4a4114b0fd4d2feb3f1c50",
      "a05a6e522f2141d5b48fcb9a77ea89a1",
      "dbbd19421df1409ca1a6891be96cf274",
      "0a977b0bbd994037b96a3efbd430218b",
      "eb3227de79f64f5b973f4f2896cadb2d",
      "f4fbaea8519f494aa9bfb5e1d0bbbc21",
      "af2a33b934c8405987d3393de0613813",
      "dff63b606c054854aed436cededa3930",
      "be2bfd431a734b479c91a38ea395a48c"
     ]
    },
    "id": "BXTODVP6uhET",
    "outputId": "986cc2cc-3487-4e63-fe83-2174561dc14e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3460227a77f41bb89a9cc2eb9733f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98671844d935406cb04c1b197b72cda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPrsPL8hH6zI",
    "outputId": "d53a364d-c511-4dab-a47a-3a8e83f1f35e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfayezrowan5\u001b[0m (\u001b[33mfayezrowan5-collage\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o7g1tQWqPvM",
    "outputId": "3e97a233-cea6-457b-f76a-a97f042cf864"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8lpJyuZ9TvO",
    "outputId": "f9ee21dc-cb1e-4b1b-bace-4b5cf20000d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:645: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "# Enable fp16 only if on a CUDA-capable GPU\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "\n",
    "# Define the training arguments (without max_length)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Directory where model predictions and checkpoints will be stored\n",
    "    learning_rate=0.0001,  # Learning rate for fine-tuning\n",
    "    num_train_epochs=1,               # Reduced epochs\n",
    "    per_device_train_batch_size=2,    # Further reduced batch size\n",
    "    per_device_eval_batch_size=2,     # Further reduced batch size\n",
    "    warmup_steps=500,                 # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                # Strength of weight decay\n",
    "    logging_dir=\"./logs\",             # Directory for storing logs\n",
    "    logging_steps=100,                # Log less frequently\n",
    "    do_train=True,                    # Enable training\n",
    "    do_eval=True,                     # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # Save model every epoch\n",
    "    save_total_limit=2,               # Keep only the last 2 checkpoints\n",
    "    load_best_model_at_end=True,      # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",     # Metric used to determine the best model\n",
    "    gradient_accumulation_steps=16,   # Gradient accumulation\n",
    "    report_to=[\"wandb\"],  # Log metrics to Weights & Biases\n",
    "    fp16=use_fp16,                    # Conditionally enable fp16 based on GPU availability\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.config.gradient_checkpointing = True  # This saves memory during training\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                        # The pre-trained model\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=tokenized_train,      # The training dataset\n",
    "    eval_dataset=tokenized_valid,       # The validation dataset\n",
    "    tokenizer=tokenizer,                # Tokenizer for preprocessing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "W2OR_OJKECP-",
    "outputId": "6c96c8c9-893f-4426-e3a7-a684e016e1f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250420_201408-r4anwv8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fayezrowan5-collage/huggingface/runs/r4anwv8j' target=\"_blank\">avid-moon-10</a></strong> to <a href='https://wandb.ai/fayezrowan5-collage/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fayezrowan5-collage/huggingface' target=\"_blank\">https://wandb.ai/fayezrowan5-collage/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fayezrowan5-collage/huggingface/runs/r4anwv8j' target=\"_blank\">https://wandb.ai/fayezrowan5-collage/huggingface/runs/r4anwv8j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2664: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.151117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=9.751897811889648, metrics={'train_runtime': 13.5059, 'train_samples_per_second': 2.665, 'train_steps_per_second': 0.074, 'total_flos': 16722690048000.0, 'train_loss': 9.751897811889648, 'epoch': 0.89})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6qKmVwNqP6K"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_gpt2_suggestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cJttcvG17ZU"
   },
   "outputs": [],
   "source": [
    "# Load the saved fine-tuned model and tokenizer\n",
    "model_path = \"./fine_tuned_gpt2_suggestions\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xxOCE2f24RX",
    "outputId": "61228145-0be5-451a-89eb-3289192b83fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl60rBq83CzT"
   },
   "outputs": [],
   "source": [
    "# Function to generate suggestions\n",
    "def generate_suggestion(prompt, max_length=50):\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZx53n-53Gmb",
    "outputId": "11cbfbe1-0375-4f0c-c210-87aa082c0fdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Suggestion:\n",
      "Students prefer multiple-choice exams because of their preference for the more standard-based, standardized exams.\n",
      "\n",
      "In addition to the general rule of thumb, students who don't like the traditional test include students with learning disabilities, who are more likely to\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Students prefer multiple-choice exams because\"\n",
    "suggestion = generate_suggestion(prompt)\n",
    "print(\"Generated Suggestion:\")\n",
    "print(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zisTOkyV3NBt",
    "outputId": "b2529288-8430-4c83-bf96-48b83525716c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Suggestion 2:\n",
      "Written exams are challenging due to the number of times they need to be completed before the final exam.\n",
      "\n",
      "The only way to ensure that you're able to achieve your dream is to have a full exam, and to keep your exams as long as\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"Written exams are challenging due to\"\n",
    "suggestion2 = generate_suggestion(prompt2)\n",
    "print(\"\\nGenerated Suggestion 2:\")\n",
    "print(suggestion2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
